{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 81707.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx_lm\n",
    "from mlx_lm.utils import load, save_config\n",
    "import pprint\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=1000)  # 1000 es el valor por defecto en muchas instalaciones de NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 77512.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2048)\n",
       "    (layers.0): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.1): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.2): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.3): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.4): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.5): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.6): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.7): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.8): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.9): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.10): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.11): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.12): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.13): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.14): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.15): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.16): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.17): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.18): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.19): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.20): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.21): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.22): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (layers.23): PhiDecoderLayer(\n",
       "      (self_attn): PhiAttention(\n",
       "        (q_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (k_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (v_proj): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (dense): Linear(input_dims=2048, output_dims=2048, bias=True)\n",
       "        (rope): RoPE(32, traditional=False)\n",
       "      )\n",
       "      (input_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "      (mlp): PhiMLP(\n",
       "        (fc1): Linear(input_dims=2048, output_dims=8192, bias=True)\n",
       "        (fc2): Linear(input_dims=8192, output_dims=2048, bias=True)\n",
       "        (act): GELU()\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm(2048, eps=1e-05, affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(input_dims=2048, output_dims=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = load(\"microsoft/phi-1.5\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.utils import load, save_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 58798.65it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"microsoft/phi-1.5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"text\": \"\\nC Read the scenario and question below, then choose the most appropriate answer from the options given (1, 2, or 3). Respond with the number of the correct answer based on the scenario's context and the question's implications.Austin raised the stake to the level of the top of the wall.\\nQ What will Austin want to do next? <answer1>sit on the wall</answer1> <answer2>tie the stake</answer2> <answer3>measure something on the wall</answer3>\\nA 2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [tokenizer.encode(sample[j]) for j in sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  198    34  4149   262  8883   290  1808  2174    11   788  3853   262\n",
      "    749  5035  3280   422   262  3689  1813   357    16    11   362    11\n",
      "    393   513   737 33556   351   262  1271   286   262  3376  3280  1912\n",
      "    319   262  8883   338  4732   290   262  1808   338 10939    13 40245\n",
      "   4376   262 10171   284   262  1241   286   262  1353   286   262  3355\n",
      "     13   198    48  1867   481  9533   765   284   466  1306    30  1279\n",
      "  41484    16    29 48937   319   262  3355  3556 41484    16    29  1279\n",
      "  41484    17    29 36224   262 10171  3556 41484    17    29  1279 41484\n",
      "     18    29  1326  5015  1223   319   262  3355  3556 41484    18    29\n",
      "    198    32   362]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=1000)  # 1000 es el valor por defecto en muchas instalaciones de NumPy\n",
    "\n",
    "print(np.array(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [198, 34, 4149, 262, 8883, 290, 1808, 2174, 11, 788, 3853, 262, 749, 5035, 3280, 422, 262, 3689, 1813, 357, 16, 11, 362, 11, 393, 513, 737, 33556, 351, 262, 1271, 286, 262, 3376, 3280, 1912, 319, 262, 8883, 338, 4732, 290, 262, 1808, 338, 10939, 13, 40245, 4376, 262, 10171, 284, 262, 1241, 286, 262, 1353, 286, 262, 3355, 13, 198, 48, 1867, 481, 9533, 765, 284, 466, 1306, 30, 1279, 41484, 16, 29, 48937, 319, 262, 3355, 3556, 41484, 16, 29, 1279, 41484, 17, 29, 36224, 262, 10171, 3556, 41484, 17, 29, 1279, 41484, 18, 29, 1326, 5015, 1223, 319, 262, 3355, 3556, 41484, 18, 29, 198, 32, 362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch =  mx.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = model(batch)\n",
    "logits = logits.astype(mx.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 111, 51200)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token = logits[:,-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlx.core.array"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(last_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.6328, 12.5703, 13.9922, ..., 3.10742, 3.10547, 3.10547]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.argmax(last_token)\n",
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġ1'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"text\": \"\\nC Read the scenario and question below, then choose the most appropriate answer from the options given (1, 2, or 3). Respond with the number of the correct answer based on the scenario's context and the question's implications.Sasha got home and took her dog for a long walk to get it some exercise.\\nQ What does Sasha need to do before this? <answer1>give the dog treats</answer1> <answer2>finish walking her dog</answer2> <answer3>grab some bags to clean up</answer3>\\nA 3\"}\n",
    "\n",
    "\n",
    "\n",
    "batch = [tokenizer.encode(sample[j]) for j in sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.array(batch)[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, tokenizer = load(\"microsoft/phi-1.5\") \n",
    "model, tokenizer = load(\"mistralai/Mistral-7B-v0.1\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28750"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch =  mx.array(batch)\n",
    "logits, _ = model(batch)\n",
    "logits = logits.astype(mx.float32)\n",
    "last_token = logits[0,-2,:]\n",
    "predictions = np.argmax(last_token)\n",
    "print(tokenizer.decode(predictions))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '3', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "k = 4  # Ajusta k según cuántos tokens superiores quieras obtener\n",
    "\n",
    "# Usar np.argsort para ordenar los índices basados en los valores de logits, tomar los últimos k elementos:\n",
    "top_indices = np.argsort(last_token)[-k:]\n",
    "\n",
    "# Decodificar cada uno de los top k indices usando el tokenizer\n",
    "decoded_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "# Imprimir los tokens decodificados\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  523, 28770, 28740, 28750])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\n",
      "answer\n",
      "3\n",
      ">\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,10):\n",
    "    last_token = logits[0,-10+i,:]\n",
    "    type(last_token)\n",
    "    predictions = np.argmax(last_token)\n",
    "    print(tokenizer.decode(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
